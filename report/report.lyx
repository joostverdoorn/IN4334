#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass IEEEtran
\begin_preamble
% for subfigures/subtables
\usepackage[caption=false,font=footnotesize]{subfig}
\end_preamble
\options journal
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding default
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command bibtex
\index_command default
\float_placement tbh
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_title "Using the StackOverflow data dump to find qualified personnel"
\pdf_author "Jasper Abbink, Joost Verdoorn"
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle false
\pdf_quoted_options "pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false"
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Using the StackOverflow data dump to find qualified personnel
\end_layout

\begin_layout Author
Jasper
\begin_inset space ~
\end_inset

Abbink,
\begin_inset space ~
\end_inset


\begin_inset Flex IEEE membership
status open

\begin_layout Plain Layout
4002237,
\end_layout

\end_inset

 Joost
\begin_inset space ~
\end_inset

Verdoorn,
\begin_inset space ~
\end_inset


\begin_inset Flex IEEE membership
status open

\begin_layout Plain Layout
1545396
\end_layout

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Jasper
\begin_inset space ~
\end_inset

Abbink is a MSc Software Technology student of Delft University of Technology,
 Faculty EEMCS, Delft, Netherlands, e-mail: 
\begin_inset CommandInset href
LatexCommand href
target "tudelft@abb.ink"
type "mailto:"

\end_inset

.
\end_layout

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Joost
\begin_inset space ~
\end_inset

Verdoorn is a MSc Software Technology student of Delft University of Technology,
 Faculty EEMCS, Delft, Netherlands, e-mail: 
\begin_inset CommandInset href
LatexCommand href
target "jverdoorn@student.tudelft.nl"
type "mailto:"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
We show that, using only open data, it is possible to construct detailed
 user profiles of active StackOverflow users.
 We cross-matched the user data provided by the StackOverflow data dump
 with other sources of user information, such as GitHub, Facebook, Twitter
 and LinkedIn.
 Recruiters of companies can use these aggregated user profiles to quickly
 find qualified potential employees to fill open job listings.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Flex Paragraph Start
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
F
\end_layout

\end_inset

inding
\end_layout

\end_inset

 qualified potential employees is big business in the IT sector.
 As the sector grows, the scarcity of skilled workers means that more and
 more resources are spent finding the right people for the job.
 We see signs of this everywhere: from the employee poaching practices now
 commonplace in Silicon Valley
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://www.forbes.com/sites/billsinger/2012/11/19/after-apple-google-adobe-pixar-g
oogle-and-intuit-antitrust-employment-charges-hit-ebay/
\end_layout

\end_inset

, to graduates of Computer Science and Computer Engineering being amongst
 the highest paid starting salaries
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://www.forbes.com/sites/susanadams/2013/09/20/the-college-degrees-with-the-hig
hest-starting-salaries/
\end_layout

\end_inset

.
 At the same time, an all-time high number of developers participate in
 what has become known as social coding: they start and contribute to open-sourc
e projects, discuss and review each others code and help each other online.
 Popular forums of these interactions, such as StackOverflow, GitHub and
 Twitter, publish some of this data.
 StackOverflow, a popular Questions & Answers website for programming-related
 questions, provides regular data dumps of the online interactions that
 occur on the website.
 Other websites have public APIs available that can be used to retrieve
 most if not all of the user's public data.
 All this data provides a wealth of information about the users, but has
 to be combined to reach its full potential.
 In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Related-Work"

\end_inset

 we will discuss related work.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Orientation"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Research-Goal"

\end_inset

 explain what our goal of this research is.
 In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Exploiting-direct-links"

\end_inset

 we will discuss what data we retrieved, and in what manner.
 In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Linking-the-data"

\end_inset

 we will show how we combined this data to build complete profiles of the
 users.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Measuring-skill"

\end_inset

 discusses what cues we can use to detect if a user is deemed skilled in
 a particular topic.
 In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:elasticsearch"

\end_inset

 we will show how we fed the data into the elasticsearch search engine to
 score and rank the assembled profiles.
 Ultimately, in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Discussion"

\end_inset

, we will discuss how our research can be used in practice, and suggest
 further research.
\end_layout

\begin_layout Section
Related Work
\begin_inset CommandInset label
LatexCommand label
name "sec:Related-Work"

\end_inset


\end_layout

\begin_layout Standard
Vasilescu et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "StackGitHub"

\end_inset

 use the StackOverflow data dump, and a similar data dump of GitHub (GHTorrent)
 to research behavioral patterns of users who use both platforms.
 They discuss how they linked the profiles from both websites, and how they
 use the combined data to show how interaction on one website influences
 or is influenced by interaction on the other.
 They find that people who commit more often on GitHub are more active at
 answering questions on StackOverflow.
 They also find that asking a question on StackOverflow often catalyzes
 committing on GitHub, suggesting that answers provided helped the user
 in finishing the commit.
\end_layout

\begin_layout Standard
Capiluppi et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Capiluppi2012"

\end_inset

 write about how to find skillful people in online communities targeted
 at nontechnical recruiters.
 They point out key metrics on Question & Answer websites (eg.
 StackOverflow), social coding websites (eg.
 GitHub, bitbucket), profile aggregators and more.
 This data is useful to our research because it gives guidance in the flood
 of data points different sites give us.
\end_layout

\begin_layout Section
Early Research
\begin_inset CommandInset label
LatexCommand label
name "sub:Orientation"

\end_inset


\end_layout

\begin_layout Standard
During our initial research we started to look at what the StackOverflow
 data contains.
 We quickly noticed that many profiles pulled their avatar images from Facebook,
 as seen in Figure
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:facebook_id_stackoverflow"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/facebook_profile_in_data.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:facebook_id_stackoverflow"

\end_inset

About 250000 StackOverflow users have their avatar images pulled from Facebook.
 The red number is a unique identifier for that user.
\end_layout

\end_inset


\end_layout

\end_inset

 As many other social media platforms, Facebook provides a public API that
 allows developers to interact with their services.
 An example of the data returned by Facebook is show in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:facebook_api_profie"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/facebook_profile_from_api.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:facebook_api_profie"

\end_inset

The profile returned by the Facebook API for the user of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:facebook_id_stackoverflow"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset

 This immediately gave us the idea to look into those links to Facebook,
 as StackOverflow profiles are usually very unpersonal and Facebook possibly
 gives more insight in the type of users that are active on StackOverflow.
 Basically the public Facebook API only provided three useful points of
 data:
\end_layout

\begin_layout Itemize
The full name of the user, splitted by first and last name(s)
\end_layout

\begin_layout Itemize
The gender
\end_layout

\begin_layout Itemize
An alias/nickname, often the same as on StackOverflow
\end_layout

\begin_layout Standard
Facebook also sometimes gives information about the birthday, but almost
 everyone has set their profile up in such a way that Facebook does not
 expose this information to us.
 Another data point we get is the locale the specific user is using the
 website of Facebook.
 We thought that to be of little use to us, since specifically software
 developers often have their locale on Facebook set to English (United States).
 
\end_layout

\begin_layout Standard
The three useful data points are unfortunately not very much and during
 our initial research we tried to detect patterns in the behavior of men
 and women on StackOverflow.
\end_layout

\begin_layout Standard
For this we applied techniques from Doing Data Science 
\begin_inset CommandInset citation
LatexCommand cite
key "dds"

\end_inset

.
 We especially tried to apply the k-Nearest Neighbors 
\begin_inset Quotes eld
\end_inset

algorithm
\begin_inset Quotes erd
\end_inset

 on the gender data point and several StackOverflow metrics, like the duration
 the user stayed active, the amount of questions and answers posted, etc.
 Unfortunately it was impossible to apply these techniques to automatically
 detect a gender.
\end_layout

\begin_layout Standard
We did however find some statistics about ordinary users, users that linked
 their account to Facebook, and differences in the gender.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/male_tags.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:male-tags"

\end_inset

Top 5 tag usage by randomly selected male users in the StackOverflow data
 dump
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/female_tags.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:female-tags"

\end_inset

Top 5 tag usage by randomly selected male users in the StackOverflow data
 dump
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:male-tags"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:female-tags"

\end_inset

 we can see the most favorite tags used by users that linked their account
 to Facebook and thereby exposed their gender to us.
 One interest immediately jumps out: Facebook users are very interested
 in programming software that integrates with the Facebook API.
 One notable difference is that male users are primarily interested in web
 development, whereas female users apparently like to develop mobile application
s.
 We compared this to the all-time top 5 tags of StackOverflow which can
 be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:general-tags"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/general_tags.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:general-tags"

\end_inset

Top 5 tag usage by all users in the StackOverflow data dump
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can see here that Java is the most popular programming language (counted
 by tag usage) on StackOverflow, but in most cases not in combination with
 the Android-tag.
 Also Facebook is completely missing from this list.
\end_layout

\begin_layout Standard
In terms of duration of activity we found out that the average female user
 stays active on StackOverflow (measured from registration date until last
 login date) for about 
\begin_inset Formula $70$
\end_inset

 days, whereas males stay active for about 
\begin_inset Formula $115$
\end_inset

 days.
 While females stay active for a shorter period of time, they do gain reputation
 on StackOverflow a little bit faster though: 
\begin_inset Formula $0.101$
\end_inset

 reputation per day for females against 
\begin_inset Formula $0.078$
\end_inset

 reputation per day for males.
\end_layout

\begin_layout Standard
As we could not find a way to automatically detect the gender of other users
 without the specific link to Facebook, we could not see if this difference
 in tags between males and females applies to all users, or is specific
 to users that signed up to StackOverflow via Facebook.
\end_layout

\begin_layout Section
Research Goal
\begin_inset CommandInset label
LatexCommand label
name "sec:Research-Goal"

\end_inset


\end_layout

\begin_layout Standard
After the previous fruitless attempt we went one step backwards to find
 a new angle to approach this data.
 Since we already requested and linked about 40000 Facebook user profiles,
 we went further into that direction and started to look for other online
 profiles that give more insight into a user.
 Via the 
\begin_inset Quotes eld
\end_inset

About Me
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

WebsiteUrl
\begin_inset Quotes erd
\end_inset

 entries in the StackOverflow profiles we could deduce many profiles from
 sites like 
\emph on
Google
\emph default
+, 
\emph on
Twitter
\emph default
, 
\emph on
LinkedIn
\emph default
 and 
\emph on
GitHub
\emph default
.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:github_in_so_profile"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/github_in_website_url.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:github_in_so_profile"

\end_inset

A user that has linked his or her GitHub account in the WebsiteUrl field
 of the StackOverflow profile.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

We saw the potential wealth of information we would have if we were able
 to link all that data together.
 Other data points these sites offer include:
\end_layout

\begin_layout Itemize
Birthday
\end_layout

\begin_layout Itemize
Programming languages a user is interested in
\end_layout

\begin_layout Itemize
Programming languages a user has proven experience with
\end_layout

\begin_layout Itemize
Hobbies
\end_layout

\begin_layout Itemize
Work history
\end_layout

\begin_layout Itemize
Education
\end_layout

\begin_layout Itemize
Knowledge of natural languages
\end_layout

\begin_layout Itemize
Preferable working hours
\end_layout

\begin_layout Standard
With this data potentially at our fingertips we had to think about what
 this could be used for.
 Since this is quite a lot of information that all these profile websites
 are giving away for free when combined, we thought that this could be ideally
 used to find good software developers, something recruiters nowadays either
 do in the traditional way (post job listings online/in papers) or pay LinkedIn
 for (can be very expensive and unfruitful).
 But when all this data is properly combined, it should be easy and cheap
 for a company to quickly find people who would be suitable for a specific
 job.
 Of course this cannot, at this moment, replace all other forms of recruitment,
 because not nearly every qualified person is active on any of those profile
 websites or they made it very hard to construct this complete identity.
 Furthermore this is only applicable to software development as there are
 no vast online communities for other markets, comparable to what StackOverflow
 is for software development.
 Still, a lot of insight could be gained by recruiters and researchers alike
 if all this data were linked together.
 
\series bold
TODO: Maybe formulate some research questions?
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Subsection
Storing the data
\end_layout

\begin_layout Standard
The entire StackOverflow data dump has a size of just below 90 gigabytes.
 In addition to this we had to account for the size of all extra data we
 scraped.
\end_layout

\begin_layout Subsubsection
Storage in MongoDB
\end_layout

\begin_layout Standard
Within MongoDB a single collection with initially the StackOverflow users
 was created.
 This collection was then augmented to also hold information about the tags
 a specific user ever used.
 Internally these were stored as a list of tags, rather than a single continuous
 string, as StackOverflow does.
\end_layout

\begin_layout Standard
Next to this augmented StackOverflow user collection, new collections were
 made for the scraped data of the other online profile websites.
 
\series bold
TODO: This was switched with Denormalisation as mongodb is really the reaon
 the data had to be denormalized.
 Check if the order is still okay.
\end_layout

\begin_layout Subsubsection
Denormalisation
\end_layout

\begin_layout Standard
Since the goal was to create something that resembles a search engine to
 find people within this data, the speed at which the data could be retrieved
 was of great importance.
 For example to get the tags, a set of keywords describing what a question
 is about on StackOverflow, a user ever used, in the original StackOverflow
 data dump one would have to get the user identification, the look all questions
 posted by this user up, and then iterate over all these questions and parse
 the tags, which are stored as a continuous string within the StackOverflow
 data dump.
 This can be very time and/or resource consuming with databases of this
 size.
 To prevent this, the decision was made to denormalize the entire StackOverflow
 data dump and store essential information we needed directly within the
 object of a user.
 Since not everyone would ever use the same tags (StackOverflow had more
 than 38.000 tags at the time the data dump was created), a relational database
 with for example one column per tag would be a weird design.
 Finally, as database software, we picked the NoSQL database MongoDB.
\end_layout

\begin_layout Subsection
Exploiting direct links
\begin_inset CommandInset label
LatexCommand label
name "sub:Exploiting-direct-links"

\end_inset


\end_layout

\begin_layout Standard
As mentioned earlier, a lot of users have manually linked their profiles
 together, usually in the 
\begin_inset Quotes eld
\end_inset

AboutMe
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

WebsiteUrl
\begin_inset Quotes erd
\end_inset

 entries of the StackOverflow data.
 Links found here were easily exploited: we wrote several simple regular
 expression to retrieve the unique identifiers present in the URLs, and
 used these identifiers to retrieve the data from public APIs.
 
\end_layout

\begin_layout Subsubsection
Facebook
\end_layout

\begin_layout Standard
Most Facebook profile links from the StackOverflow data dump come from the
 avatars, an image identifying a user on a website, of a user profile.
 Since all accounts linked to Facebook automatically get their Facebook
 profile picture set as avatar on StackOverflow, these URLs are easy to
 spot.
 These URLs are also in a standardized format and contain the internal numeric
 identifier of the Facebook profile.
 This identifier can directly be used in a search query to the public Facebook
 API.
 Unfortunately Facebook has a limit of 600 requests per ten minutes for
 non-paying customers.
 This substantially reduces the speed at which downloading this information
 is possible.
\end_layout

\begin_layout Standard
Also, as pointed out in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Orientation"

\end_inset

, Facebook hardly gives away any information to non-paying customers, whereas
 the other services are more willingly to disclose private information of
 their users.
 Also, we mentioned in that section that we didn't have any use for the
 language setting of the user.
 That is no longer the case: although the locale is often set to the default,
 American English, it can be another hint of the user's location if it is
 set to a non-default value.
 
\end_layout

\begin_layout Subsubsection
GitHub
\end_layout

\begin_layout Standard
The links to GitHub users are easy to see in the 
\begin_inset Quotes eld
\end_inset

About Me
\begin_inset Quotes erd
\end_inset

 section on StackOverflow.
 They also comply to a default format for the URL which allows for an easy
 extraction of the GitHub user name from the URL.
 To not reinvent the wheel, we looked for other options to quickly gain
 access to the user profiles on GitHub.
 Luckily there is already a project specialized in this and with the help
 of GHTorrent 
\begin_inset CommandInset citation
LatexCommand cite
key "Gousi13"

\end_inset

 we quickly had access to an almost complete copy of all GitHub data.
 Since initially we would only focus on aspects of users, only that database
 was of interest to us.
 Later we used the GitHub API to retrieve repositories users contributed
 to 
\end_layout

\begin_layout Subsubsection
Twitter, LinkedIn, Google+
\end_layout

\begin_layout Standard
Links to the three social network profile sites 
\emph on
Twitter
\emph default
, 
\emph on
LinkedIn
\emph default
 and 
\emph on
Google+
\emph default
 were found just like the GitHub accounts, but for all three tools had to
 be written to actually retrieve the data as there are no products like
 GHTorrent that work with these sites.
\end_layout

\begin_layout Subsection
Linking the data
\begin_inset CommandInset label
LatexCommand label
name "sub:Linking-the-data"

\end_inset


\end_layout

\begin_layout Subsubsection
Name and location based matching
\end_layout

\begin_layout Standard
Although we now had a decent amount of linked profiles from multiple platforms,
 most users did not have any manually linked profiles, aside from the Facebook
 profile images.
 We suspect that most users link their Facebook profile image unknowingly,
 because the number of linked profile images (about 250000) is substantially
 larger than any other linked information.
 We decided to attempt to link more data in other ways.
 In the research of Vasilescu et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "StackGitHub"

\end_inset

 methods are described to reliably create intersections between (specifically)
 the StackOverflow data and the GitHub data.
 Unfortunately for us, they make use of the email address (a Message Digest
 5 hash of the email address), which StackOverflow does not expose anymore
 since that paper was published.
 Therefore we had to fall back to other methods as described by Bird et
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Bird:2006:MES:1137983.1138033"

\end_inset

 and Kouters et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Gnome"

\end_inset

.
 The first one tries to match full names or email addresses shared by different
 people in communities.
 The second one uses Latent Semantic Analysis (LSA), an information retrieval
 technique that works good with noisy data.
 These approaches do suffer from false negatives and positives, though.
 
\end_layout

\begin_layout Standard
However, we found there are more ways to reliably link profiles.
 Apart from the username (stored as 
\begin_inset Quotes eld
\end_inset

DisplayName
\begin_inset Quotes erd
\end_inset

 in the StackOverflow data dump), a lot of records also contain a 
\begin_inset Quotes eld
\end_inset

Location
\begin_inset Quotes erd
\end_inset

 field.
 We also had access to a lot of real names provided by the Facebook profiles.
 Although matching on only the username or full name may result in a lot
 of false positive matches, as multiple users might use the same username
 on different platforms, and real names aren't globally unique either, a
 small qualitative research showed that when we combined the username, real
 name and location fields, we would get virtually no false positive matches
 when cross-linking the profiles with the GHTorrent data dump.
 Therefore we have chosen to try to find matches in profiles with these
 three data points and assume a profile on StackOverflow belongs to the
 same person as a profile on GitHub when two of these points match (eg.
 full name and username, or username and location).
 There is of course still a slim chance that within the same city two people
 have exactly the same name, but this is inevitable and should not occur
 very often.
 In this manner, we found about 4000 additional links between StackOverflow
 and GitHub.
\end_layout

\begin_layout Subsubsection
Mining coordinates
\end_layout

\begin_layout Standard
Another data point many sites have is the location of the user.
 Together with an online service to convert this to coordinates on the globe
 (we used Microsoft Bing for this due to the high request limit they offer)
 insights can be gained about the specific location of the user, assuming
 this was entered correctly.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Location"

\end_inset

 will explain in more detail why having this location is a useful metric
 in our search engine.
\end_layout

\begin_layout Subsection
Searching within the data
\begin_inset CommandInset label
LatexCommand label
name "sub:elasticsearch"

\end_inset


\end_layout

\begin_layout Standard
To be able to search, score and rank our data, the open-source search engine
 elasticsearch was used.
 The usage of elasticsearch has several benefits: although it is possible
 to use MongoDB's MapReduce queries for searching and especially scoring
 the data, elasticsearch was built for this purpose.
 elasticsearch makes it very easy to create custom scoring functions for
 many types of queries.
 Besides that, elasticsearch is great at fuzzy searches, which for example
 makes it easier to approximately match strings representing languages and
 technologies.
 Using the MongoDB River extension for elasticsearch we were able to have
 the elasticsearch index update whenever the data in MongoDB was changed.
 
\end_layout

\begin_layout Standard
We also fed the GHTorrent data into ElasticSearch, which allowed us to attempt
 to link users in a more dynamic fashion.
 In a short qualitative research fase, we defined a set of fuzzy search
 queries by which we should deduct that two profiles from different websites
 are the same, be it they pass a certain score.

\series bold
 
\series default
We found, however, that fuzzy matching, although good at detecting matches,
 also has a very high amount of false positives which were hard to get rid
 of.
 We therefore decided not to pursue this any further.
\end_layout

\begin_layout Section
Scoring and ranking
\begin_inset CommandInset label
LatexCommand label
name "sub:Measuring-skill"

\end_inset


\end_layout

\begin_layout Standard
Now that we have all our data neatly indexed by elasticsearch, we can determine
 scoring functions to rank users according to their profiles.
 Cappiluppi et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Capiluppi2012"

\end_inset

 state what aspects of social media may be important for assessing candidates,
 and what to avoid.
 We define 
\begin_inset Quotes eld
\end_inset

cues
\begin_inset Quotes erd
\end_inset

 as a hints of a certain desirable trait in candidates, which in themselves
 may not mean a lot but when put together will build a pretty complete image
 of the user.
 We identify a number cues for a number of sought-after attributes in candidates
, and how they may be retrieved from the data.
 It is then up to a recruiter to determine how important each aspect is.
\end_layout

\begin_layout Subsection
Language proficiency
\end_layout

\begin_layout Standard
One obvious aspect of scoring candidates involves language proficiency.
 Although it's hard to automatically detect good code from bad code, a user's
 contribution to popular repositories that are written in a certain language
 can be seen as an important cue for a user's proficiency in that language.
 Another important cue is the use of post tags on StackOverflow.
 These tags often describe the language the post is about, and be it a question
 or an answer, it hints at a knowledge of that language.
\end_layout

\begin_layout Subsection
Framework knowledge
\end_layout

\begin_layout Standard
Recruiters also often look for experience in popular frameworks.
 Although apart from direct contributions to these frameworks it may be
 hard to retrieve whether or not a user has used a particular framework,
 StackOverflow activity may hint toward the usage.
 Besides that, it's possible to use data retrieved Google+ and Twitter to
 detect interest in a certain framework.
\end_layout

\begin_layout Subsection
Location
\begin_inset CommandInset label
LatexCommand label
name "sub:Location"

\end_inset


\end_layout

\begin_layout Standard
Another obvious selection metric is the location of a user.
 As we can pull in coordinates of the users, we can actually define the
 radius in which the candidate must live in order to be eligible.
 However, most users do not have a location set in one of their profiles.
 Twitter, however, often includes a geographical locations in its tweets
 which we can use.
 For non-US locations, we can also use the language data retrieved from
 Facebook as a hint a user might be in a certain geographical area.
\end_layout

\begin_layout Subsection
Helpfulness
\end_layout

\begin_layout Standard
Helpfulness is an obvious plus in candidates, and cues for a candidate's
 helpfulness can be retrieved directly from StackOverflow.
 A helpful person will answer a relatively high amount of questions.
\end_layout

\begin_layout Subsection
Team player
\end_layout

\begin_layout Standard
A team player contributes and communicates on GitHub.
 Cues for finding team players could be the discussion taking place on GitHub,
 and the number of multi-person repositories a person contributes to.
\end_layout

\begin_layout Subsection
Community builder
\end_layout

\begin_layout Standard
A community builder is pretty easy to spot.
 He has much reputation on StackOverflow, many watchers on GitHub and/or
 many followers on Twitter.
 He may also have a few popular repositories under his name.
 
\end_layout

\begin_layout Subsection
Technology enthousiast
\end_layout

\begin_layout Standard
It is often found important that developers keep themselves up-to-date with
 the latest and greatest technology, and gets enthoused about it.
 Cues for this can be found on the more social platforms, such as Twitter
 and Google+.
 We can score users based upon who they follow on Twitter, or what communities
 they're part of on Google+, although identifying the right people to follow
 and communities to be a part of deserves a study of it's own.
 One way to identify them is finding common patterns in the Twitter and
 Google+ behaviours of developers.
\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
There are multiple aspects of our approach that are sub-optimal.
\end_layout

\begin_layout Subsection
Denormalisation
\end_layout

\begin_layout Standard
The first is that we denormalise all data.
 This makes it easier to search through all data, but denormalising the
 data first takes more time.
 Also when there are new data dumps of StackOverflow, which is the starting
 point of our database, it will be hard to incorporate all changes with
 our current setup.
 This could either be circumvented by trying to get only the differences
 between the most recent and second to last data dump of StackOverflow,
 or we have to think about a way to not denormalise the data as we do now.
\end_layout

\begin_layout Subsection
Scraping other sites
\end_layout

\begin_layout Standard
The availability of the StackOverflow data via their official data dumps
 is very useful in this context, but other data providers are not very willing
 to give all their data away at once.
 Luckily for GitHub we have access to the third-party project GHTorrent,
 but for other sites projects like this do not exist at this moment.
 Some of them, especially Facebook, even have very hard restrictions to
 how many requests to their API can be made per given time frame.
 For example for Facebook it would take several days just to retrieve all
 profiles as they are now and there is no way to know when this data is
 outdated again.
 Twitter, LinkedIn and Google+ also have limits to their API, although higher
 than Facebook, but they allow for more requests per second.
 Again, for these sites we have no ability to know when a profile was updated
 which makes keeping our data set up to date infeasible, as we would have
 to request all profiles repeatedly over time.
 This is probably something the operators of those sites would not like
 us to do and it is also quite counterproductive.
\end_layout

\begin_layout Subsection
Speed
\begin_inset CommandInset label
LatexCommand label
name "sub:Speed"

\end_inset


\end_layout

\begin_layout Standard
Another aspect that would need attention is the speed at which search queries
 run now.
 To quickly find people satisfying certain search criteria, we still need
 to iterate over all entries in the database as of this moment.
 We added elasticsearch because that engine is usually better in these kind
 of search queries than querying directly MongoDB, but also elasticsearch
 can be quite slow.
 This happens because we use scoring functions for certain keywords in for
 example the used tags.
 This is information elasticsearch does not have indexed as of this moment
 in our implementation.
 In practice elasticsearch is only really fast when used with filtering
 and faceted search.
\end_layout

\begin_layout Subsection
Practical use
\end_layout

\begin_layout Standard
We have not spoken to potential users of this search engine during the developme
nt of it.
 Therefore we do not exactly know how recruiters would like to filter their
 potential candidates as of this moment.
 Knowing this possibly makes it easier to structure the data in such a way
 that the problems described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Speed"

\end_inset

 can be addressed.
 Another possibility would be that the data points we index now are not
 actually what recruiters are interested in.
 This could mean that we need other data sources than the ones we chose
 now (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Sources"

\end_inset

).
\end_layout

\begin_layout Subsection
Sources
\begin_inset CommandInset label
LatexCommand label
name "sub:Sources"

\end_inset


\end_layout

\begin_layout Standard
We started this research on profile creation with the users of StackOverflow
 because we quickly noticed that many people on that site have their account
 linked to a Facebook account.
 From there on, we searched through the entire data set for the use of more
 social networks.
 From that information we chose to include Twitter, LinkedIn, GitHub and
 Google+ as well.
 There is of course the possibility that there are more suitable sources
 for data.
 Some of these are discussed by Capiluppi et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Capiluppi2012"

\end_inset

.
 One interesting source they name is bitbucket, a competitor to GitHub.
 Unfortunately especially bitbucket is mainly used for closed-source projects
 which we could not get access to.
 To see how popular these services actually are, we went queried the Google
 Search statistics and came up with the graph shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Search-trends-on"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/github_bitbucket_googlecode_sourceforge.png
	scale 80
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Search-trends-on"

\end_inset

Search trends on Google Search for the keywords 
\begin_inset Quotes eld
\end_inset

GitHub
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Bitbucket
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Google Code
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

SourceForge
\begin_inset Quotes erd
\end_inset

 from January 2004 - January 2015
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can see from this graph, according to the search interest of the users
 of Google Search, SourceForge is a dying project.
 Also even though bitbucket is growing, it is still tiny compared to both
 GitHub and Google Code.
 Also as of January 2015 GitHub surpassed Google Code in popularity.
 Judging from this data it seems like a good idea to also try to match people
 from StackOverflow to contributions on Google Code projects.
\end_layout

\begin_layout Standard
Other sources named by Capiluppi et al.
 are so called profile aggregation sites.
 These sites either automatically scrape all kinds of data about users from
 all over the internet, or they allow users to specifically create their
 own profile and link all remote websites themselves.
 The good thing is, that sites doing this specifically for software development
 exist (eg.
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://masterbranch.com
\end_layout

\end_inset

, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://coderwall.com
\end_layout

\end_inset

, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://geekli.st
\end_layout

\end_inset

).
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
In this paper we studied the feasibility of creating a search engine capable
 of finding skillful software developers using only open data sources.
 We found that specifically for software development this is indeed possible,
 thanks to the big Question & Answer site StackOverflow and the upcoming
 phenomenon 
\begin_inset Quotes eld
\end_inset

social coding
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Starting with the user database of StackOverflow we started retrieving the
 linked Facebook profiles.
 From there on we also started to look up all links to other popular social
 networking sites.
 For GitHub we made use of the unofficial data dumps made available by GHTorrent.
 This significantly sped up the process of finding GitHub's users.
 After retrieving the user profiles from the other sites we could then link
 profiles based on real name, full name and location.
 These aggregated profiles then contained everything we could find about
 a user, including personal details like the name but also the work history,
 education and favorite programming languages.
\end_layout

\begin_layout Standard
While implementing this system we found however that more research is required
 to really make a free alternative to LinkedIn Premium (for example).
 For one thing we should rethink which data is actually important.
 Researching this with the help of people that would actually use this tool
 (recruiters) is probably the best path to take.
 When these priorities are defined, this gained knowledge should be applied
 to the data structure to ensure decent performances while using the system.
 This new data structure should also prevent the degree of denormalisation
 we face now; this denormalisation prevents keeping the data set up to date.
\end_layout

\begin_layout Appendices
\begin_inset Note Note
status open

\begin_layout Plain Layout
Don't add text here!
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibtex"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
