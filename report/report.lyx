#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass IEEEtran
\begin_preamble
% for subfigures/subtables
\usepackage[caption=false,font=footnotesize]{subfig}
\end_preamble
\options journal
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding default
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command bibtex
\index_command default
\float_placement tbh
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_title "Using the StackOverflow data dump to find qualified personnel"
\pdf_author "Jasper Abbink, Joost Verdoorn"
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle false
\pdf_quoted_options "pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false"
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Using the StackOverflow data dump to find qualified personnel
\end_layout

\begin_layout Author
Jasper
\begin_inset space ~
\end_inset

Abbink,
\begin_inset space ~
\end_inset


\begin_inset Flex IEEE membership
status open

\begin_layout Plain Layout
4002237,
\end_layout

\end_inset

 Joost
\begin_inset space ~
\end_inset

Verdoorn,
\begin_inset space ~
\end_inset


\begin_inset Flex IEEE membership
status open

\begin_layout Plain Layout
1545396
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Jasper
\begin_inset space ~
\end_inset

Abbink is a MSc Software Technology student of Delft University of Technology,
 Faculty EEMCS, Delft, Netherlands, e-mail: 
\begin_inset CommandInset href
LatexCommand href
target "tudelft@abb.ink"
type "mailto:"

\end_inset

.
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Joost
\begin_inset space ~
\end_inset

Verdoorn is a MSc Software Technology student of Delft University of Technology,
 Faculty EEMCS, Delft, Netherlands, e-mail: 
\begin_inset CommandInset href
LatexCommand href
target "jverdoorn@student.tudelft.nl"
type "mailto:"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
We show that, using only open data, it is possible to construct detailed
 profiles StackOverflow users.
 We crossmatched the user data provided by the StackOverflow data dump with
 other sources of user information, such as GitHub, Facebook and Twitter.
 ...
 MORE TODO
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Flex Paragraph Start
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
F
\end_layout

\end_inset

inding
\end_layout

\end_inset

 qualified potential employees is big bussiness in the IT sector.
 As the sector grows, the scarcity of skilled workers means that more and
 more resources are spent finding the right people for the job.
 At the same time, an all-time high number of developers participate in
 what has become known as social coding: they start open-source projects,
 discuss code and help each other online.
 Popular forums of these interactions, such as StackOverflow, GitHub and
 Twitter, have APIs available that can be used to retrieving this data for
 analysis.
 
\end_layout

\begin_layout Section
Related Work
\end_layout

\begin_layout Standard
TODO: text text text text text text text text text text text text text text
 text
\end_layout

\begin_layout Subsection
subsection
\end_layout

\begin_layout Subsection
another subsection
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
As we only received the StackOverflow database, one part of the research
 was to find interesting aspects in this data ourselves.
 
\end_layout

\begin_layout Subsection
Orientation
\begin_inset CommandInset label
LatexCommand label
name "sub:Orientation"

\end_inset


\end_layout

\begin_layout Standard
During our initial research we started to look at what the StackOverflow
 data contains.
 We quickly noticed that many profiles were linked to a Facebook account.
 This immediately gave us the idea to look into those links to Facebook,
 as StackOverflow profiles are usually very unpersonal and Facebook possibly
 gives more insight in the type of users that are active on StackOverflow.
 Basically the public Facebook API only provided three useful points of
 data:
\end_layout

\begin_layout Itemize
The full name of the user, splitted by first and last name(s)
\end_layout

\begin_layout Itemize
The gender
\end_layout

\begin_layout Itemize
An alias/nickname, usually the same as on StackOverflow
\end_layout

\begin_layout Standard
Unfortunately this was not very much and during our initial research we
 tried to detect patterns in the behaviour of men and women on StackOverflow.
\end_layout

\begin_layout Standard
For this we applied techniques from Doing Data Science 
\begin_inset CommandInset citation
LatexCommand cite
key "dds"

\end_inset

.
 We especially tried to apply the k-Nearest Neighbors 
\begin_inset Quotes eld
\end_inset

algorithm
\begin_inset Quotes erd
\end_inset

 on the gender datapoint and several StackOverflow metrics, like the duration
 the user stayed active, the amount of questions and answers posted, etc.
 Unfortunately it was impossible to find any indication that gender actually
 had any influence at all.
\end_layout

\begin_layout Subsection
Re-orientation
\end_layout

\begin_layout Standard
After this failed attempt we went one step backwards to find a new angle
 to approach this data.
 Since we already downloaded many Facebook user profiles, we went further
 into that direction and started to look for other online profiles that
 give more insight into a user.
 Via the 
\begin_inset Quotes eld
\end_inset

About Me
\begin_inset Quotes erd
\end_inset

 section of the StackOverflow profiles we could deduce many profiles from
 sites like 
\emph on
Google
\emph default
+, 
\emph on
Twitter
\emph default
, 
\emph on
LinkedIn
\emph default
 and 
\emph on
GitHub
\emph default
.
 All these sites give at least the name of the user and possibly hold a
 different online alias or username for that person.
 This online alias helps to link to profiles on other sites that the user
 on StackOverflow did not explicitly specified.
\end_layout

\begin_layout Standard
Another data point many sites have is the location of the user.
 Together with an online service to convert this to coordinates on the globe
 (we used Microsoft Bing for this due to the high request limit they offer)
 insights can be gained about the specific location of the user, assuming
 this was entered correctly.
\end_layout

\begin_layout Standard
Other data points these sites offer include:
\end_layout

\begin_layout Itemize
Birthday
\end_layout

\begin_layout Itemize
Programming languages a user is interested in
\end_layout

\begin_layout Itemize
Programming languages a user has proven experience with
\end_layout

\begin_layout Itemize
Hobbies
\end_layout

\begin_layout Itemize
Work history
\end_layout

\begin_layout Itemize
Education
\end_layout

\begin_layout Itemize
Knowledge of natural languages
\end_layout

\begin_layout Itemize
Preferable working hours
\end_layout

\begin_layout Subsection
Goal
\end_layout

\begin_layout Standard
With this data potentially at our fingertips we had to think about what
 this could be used for.
 Since this is quite a lot of information that all these profile websites
 are giving away for free when combined, we thought that this could be ideally
 used to find good software developers, something recruiters nowadays either
 do in the traditional way (post job listings online/in papers) or pay LinkedIn
 for (can be very expensive and unfruitful).
 But when all this data is properly combined, it should be easy and cheap
 for a company to quickly find people who would be suitable for a specific
 job.
 Of course this cannot, at this moment, replace all other forms of recruitment,
 because not every smart person is active on any of those profile websites
 or they made it very hard to construct this complete identity.
 Furthermore this is only applicable to software development as there are
 no vast online communities for other markets, comparable to what StackOverflow
 is for software development.
\end_layout

\begin_layout Subsection
Mining more data
\end_layout

\begin_layout Standard
The StackOverflow database is available as a single download, but the data
 from (most) other sources are not.
 Getting access to this data was the next step.
\end_layout

\begin_layout Subsubsection
Facebook
\end_layout

\begin_layout Standard
Most Facebook profiles from the StackOverflow data dump come from the avatar,
 an image identifying a user on a website, of a user profile.
 Since all accounts linked to Facebook automatically get their Facebook
 profile picture set as avatar on StackOverflow, these URL's are easy to
 spot.
 These URL's are also in a standardized format and contain the internal
 numeric identifier of the Facebook profile.
 This identifier can directly be used in a search query to the public Facebook
 API.
 Unfortunately Facebook has a limit of 600 requests per ten minutes for
 non-paying customers.
 This substantially reduces the speed at which downloading this information
 is possible.
\end_layout

\begin_layout Standard
Also, as pointed out in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Orientation"

\end_inset

, Facebook hardly gives away any information to non-paying customers, whereas
 the other services are more willingly to disclose private information of
 their users.
\end_layout

\begin_layout Subsubsection
GitHub
\end_layout

\begin_layout Standard
The links to GitHub users are easy to see in the 
\begin_inset Quotes eld
\end_inset

About Me
\begin_inset Quotes erd
\end_inset

 section on StackOverflow.
 They also comply to a default format for the URL which allows for an easy
 extraction of the GitHub username from the URL.
 To not reinvent the wheel, we looked for other options to quickly gain
 access to the user profiles on GitHub.
 Luckily there is already a project specialised in this and with the help
 of GHTorrent 
\begin_inset CommandInset citation
LatexCommand cite
key "Gousi13"

\end_inset

 we quickly had access to an almost complete copy of all GitHub data.
 Since we would only focus on aspects of users, only that database was of
 interest to us.
\end_layout

\begin_layout Subsubsection
Twitter, LinkedIn, Google+
\end_layout

\begin_layout Standard
Links to the three social network profile sites 
\emph on
Twitter
\emph default
, 
\emph on
LinkedIn
\emph default
 and 
\emph on
Google+
\emph default
 were found just like the GitHub accounts, but for all three tools had to
 be written to actually retrieve the data as there are no products like
 GHTorrent that work with these sites.
\end_layout

\begin_layout Subsection
Storing the data
\end_layout

\begin_layout Standard
The entire StackOverflow data dump has a size of just below 90 gigabytes.
 In addition to this we had to account for the size of all extra data we
 scraped.
\end_layout

\begin_layout Subsubsection
Denormalisation
\end_layout

\begin_layout Standard
Since the goal was to create something that resembles a search engine to
 find people within this data, the speed at which the data could be retrieved
 was of great importance.
 For example to get the tags, a set of keywords describing what a question
 is about on StackOverflow, a user ever used, in the original StackOverflow
 data dump one would have to get the user identification, the look all questions
 posted by this user up, and then iterate over all these questions and parse
 the tags, which are stored as a continous string within the StackOverflow
 data dump.
 This can be very time and/or resource consuming with databases of this
 size.
 To prevent this, the decision was made to denormalize the entire StackOverflow
 data dump and store essential information we needed directly within the
 object of a user.
 Since not everyone would ever use the same tags (StackOverflow had more
 than 38.000 tags at the time the data dump was created), a relational database
 with for example one column per tag would be a weird design.
 Finally, as database software, we picked the NoSQL database MongoDB.
\end_layout

\begin_layout Subsubsection
Storage in MongoDB
\end_layout

\begin_layout Standard
Within MongoDB a single collection with initially the StackOverflow users
 was created.
 This collection was then augmented to also hold information about the tags
 a specific user ever used.
 Internally these were stored as a list of tags, rather than a single continous
 string, as StackOverflow does.
\end_layout

\begin_layout Standard
Next to this augmented StackOverflow user collection, new collections were
 made for the scraped data of the other online profile websites.
\end_layout

\begin_layout Subsubsection
Linking the profiles
\end_layout

\begin_layout Standard
In the research of Vasilescu et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "StackGitHub"

\end_inset

 methods are described to reliably create intersections between (specifically)
 the StackOverflow data and the GitHub data.
 Unfortunately they make use of the email address (and hashed versions of
 the email address), which StackOverflow does not expose anymore since the
 paper was published.
 Therefore we had to fall back to other methods as described by Bird et
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Bird:2006:MES:1137983.1138033"

\end_inset

 and Kouters et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Gnome"

\end_inset

.
 The first one tries to match full names or email adresses shared by different
 people in communities.
 The second one uses Latent Semantic Analysis (LSA), an information retrieval
 technique that works good with noisy data.
 These approaches do suffer from false negatives and positives, though.
 To prevent pollution by these false positives, we try to match users conservati
vely by ignoring users with (English) dictionary words as username.
\end_layout

\begin_layout Standard
In addtion to this matching by username we have more options to link users.
 The location and full name data points are also good indicators about the
 identity of a person.
 Therefore we have chosen to try to find matches in profiles with these
 three data points and assume a person on site A is the same as on site
 B when two of these points match (eg.
 full name and username).
 There is of course still a slim chance that within the same city two people
 have exactly the same name, but this is inevitable and should not occur
 very often.
\end_layout

\begin_layout Subsection
Searching within the data
\end_layout

\begin_layout Standard
To be able to search, score and rank our data, the open source search engine
 ElasticSearch was used.
 The usage of ElasticSearch has several benefits: although it is possible
 to use MongoDB's MapReduce queries for searching and scoring the data,
 ElasticSearch was built for this purpose.
 Besides that, ElasticSearch is great at fuzzy searches, which makes it
 easier to approximately match strings representing languages and technologies.
 Using the MongoDB River extension for ElasticSearch we were able to have
 the ElasticSearch index update whenever the data in MongoDB was changed.
 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\family sans
A single column figure goes here
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Captions go 
\emph on
under
\emph default
 the figure
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Table captions go 
\emph on
above
\emph default
 the table
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
delete
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
this
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
example
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
table
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
There are multiple aspects of our approach that are sub-optimal.
\end_layout

\begin_layout Subsection
Denormalisation
\end_layout

\begin_layout Standard
The first is that we denormalise all data.
 This makes it easier to search through all data, but denormalising the
 data first takes more time.
 Also when there are new data dumps of StackOverflow, which is the starting
 point of our database, it will be hard to incorporate all changes with
 our current setup.
 This could either be circumvented by trying to get only the differences
 between the most recent and second to last data dump of StackOverflow,
 or we have to think about a way to not denormalise the data as we do now.
\end_layout

\begin_layout Subsection
Scraping other sites
\end_layout

\begin_layout Standard
The availability of the StackOverflow data via their official data dumps
 is very useful in this context, but other data providers are not very willing
 to give all their data away at once.
 Luckily for GitHub we have access to the third-party project GHTorrent,
 but for other sites projects like this do not exist at this moment.
 Some of them, especially Facebook, even have very hard restrictions to
 how many requests to their API can be made per given timeframe.
 For example for Facebook it would take several days just to retrieve all
 profiles as they are now and there is no way to know when this data is
 outdated again.
 Twitter, LinkedIn and Google+ also have limits to their API, although higher
 than Facebook, but they allow for a more requests per second.
 Again, for these sites we have no ability to know when a profile was updated
 which makes keeping our data set up to date infeasible, as we would have
 to request all profiles repeatedly over time.
 This is probably something the operators of those sites would not like
 us to do and it is also quite counterproductive.
\end_layout

\begin_layout Subsection
Speed
\begin_inset CommandInset label
LatexCommand label
name "sub:Speed"

\end_inset


\end_layout

\begin_layout Standard
Another aspect that would need attention is the speed at which search queries
 run now.
 To quickly find people satisfying certain search criteria, we still need
 to iterate over all entries in the database as of this moment.
 We added elasticsearch because that engine is usually better in these kind
 of search queries than querying directly MongoDB, but also elasticsearch
 can be quite slow.
 This happens because we use scoring functions for certain keywords in for
 example the used tags.
 This is information elasticsearch does not have indexed as of this moment
 in our implementation.
 In practice elasticsearch is only really fast when used with filtering
 and facetted search.
\end_layout

\begin_layout Subsection
Practical use
\end_layout

\begin_layout Standard
We have not spoken to potential users of this search engine during the developme
nt of it.
 Therefore we do not exactly know how recruiters would like to filter their
 potential candidates as of this moment.
 Knowing this possibly makes it easier to structure the data in such a way
 that the problems described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Speed"

\end_inset

 can be adressed.
 Another possibility would be that the data points we index now are not
 actually what recruiters are interested in.
 This could mean that we need other data sources than the ones we chose
 now (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Sources"

\end_inset

).
\end_layout

\begin_layout Subsection
Sources
\begin_inset CommandInset label
LatexCommand label
name "sub:Sources"

\end_inset


\end_layout

\begin_layout Standard
We started this research on profile creation with the users of StackOverflow
 because we quickly noticed that many people on that site have their account
 linked to a Facebook account.
 From there on, we searched through the entire data set for the use of more
 social networks.
 From that information we chose to include Twitter, LinkedIn, GitHub and
 Google+ as well.
 There is of course the possibility that there are more suitable sources
 for data.
 Some of these are discussed by Capiluppi et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Capiluppi2012"

\end_inset

.
 One interesting source they name is bitbucket, a competitor to GitHub.
 Unfortunately especially bitbucket is mainly used for closed-source projects
 which we could not get access to.
 To see how popular these services actually are, we went queried the Google
 Search statistics and came up with the graph shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Search-trends-on"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/github_bitbucket_googlecode_sourceforge.png
	scale 40
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Search-trends-on"

\end_inset

Search trends on Google Search for the keywords 
\begin_inset Quotes eld
\end_inset

GitHub
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Bitbucket
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Google Code
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

SourceForge
\begin_inset Quotes erd
\end_inset

 from January 2004 - January 2015
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can see from this graph, according to the search interest of the users
 of Google Search, SourceForge is a dying project.
 Also even though bitbucket is growing, it is still tiny compared to bothe
 GitHub and Google Code.
 Also as of January 2015 GitHub surpassed Google Code in popularity.
 Judging from this data it seems like a good idea to also try to match people
 from StackOverflow to contributions on Google Code projects.
\end_layout

\begin_layout Standard
Other sources named by Capiluppi et al.
 are so called profile aggregation sites.
 These sites either automatically scrape all kinds of data about users from
 all over the internet, or they allow users to specifically create their
 own profile and link all remote websites themselves.
 The good thing is, that sites doing this specifically for softtware development
 exist (eg.
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://masterbranch.com
\end_layout

\end_inset

, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://coderwall.com
\end_layout

\end_inset

, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://geekli.st
\end_layout

\end_inset

).
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
bla bla TODO
\end_layout

\begin_layout Appendices
\begin_inset Note Note
status open

\begin_layout Plain Layout
Don't add text here!
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibtex"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
